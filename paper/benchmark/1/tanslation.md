# Understanding Social Reasoning in Language Models with Language Models
As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.
随着大型语言模型（LLM）越来越多地融入我们的日常生活，了解它们理解人类心理状态的能力对于确保有效的交互变得至关重要。然而，尽管最近尝试评估LLM的心智理论 (ToM) 推理能力，但这些模型与人类心智理论 (ToM) 的一致性程度仍然是一个微妙的探索话题。这主要是由于两个不同的挑战：（1）以前的评估结果不一致，（2）对现有评估方法有效性的担忧。为了应对这些挑战，我们提出了一个新颖的框架，通过populate casual templates来程序化地生成法学硕士的评估。使用我们的框架，我们为LLM创建了一个新的社会推理基准 **(BigToM)**，其中包含 25 个对照和* 5,000 个模型编写的评估。我们发现，人类参与者对我们基准的质量评价高于以前的crowded-sourced评估，并且与专家撰写的评估相当。使用 BigToM，我们评估了各种法学硕士的社会推理能力，并将模型表现与人类表现进行比较。我们的结果表明，GPT4 具有反映人类推理模式的 ToM 功能，尽管不太可靠，而其他LMM则苦苦挣扎。

Humans continually try to understand what others think, want, and feel.
人类不断尝试了解他人的想法、需求和感受。

We try to understand what people have done and predict what they might do next by inferring their mental states. This capability, often referred to as “Theory of Mind” (ToM), is the foundation of social interaction [45, 22, 25, 10, 38]. With Large Language Models (LLMs) playing a growing role in our lives, assessing their ability to model human mental states is key for guaranteeing effective interactions. This involves evaluating the current abilities of LLMs, understanding their failure modes, and discovering ways to improve them. LLMs with ToM-like abilities could be better at teaching us, learning from us, communicating with us, collaborating with us, and understanding us [15, 20, 30, 11, 36].


我们试图通过推断人们的心理状态来了解他们做了什么并预测他们下一步可能会做什么。这种能力通常被称为“心智理论”（ToM），是社交互动的基础[45,22,25,10,38]。随着大型语言模型 (LLM) 在我们的生活中发挥着越来越重要的作用，评估其模拟人类心理状态的能力是保证有效交互的关键。这涉及评估LLM的当前的能力，了解他们的失败模式，并发现改进方法。具有类似 ToM 能力的法学硕士可以更好地教导我们、向我们学习、与我们沟通、与我们合作以及理解我们 [15,20,30,11,36]。

Recent attempts at understanding social reasoning in LLMs have used crowd-sourced data, SocialIQA [32], data from synthetic templates, ToMi [21], or (modified) tests from psychology designed to evaluate human capabilities [e.g. 24, 42, 18, 5, 23, 41]. Sap et al. [33] used SocialIQA and ToMi to show that GPT-3 had limitied social reasoning capabilities. However, their findings are challenging to interpret due to limitations in their methodology. SocialIQA has several ambiguous examples and stories that do not effectively test the desired social reasoning behaviors. In comparison, ToMi suffers from ambiguous narratives with unclear perceptual descriptions and additional confounding factors in reasoning like memory loads or tracking requirements. Moreover, both of these datasets lack control conditions making it difficult to identify precisely where models make mistakes. The results of studies with tests developed by psychologists show some signs of ToM capabilites in LLMs。
However, when LLMs such as GPT-3 [4] succeed in scenarios, they often fail dramatically on trivial alterations [42, 24, 35]. Despite their careful design, concerns about the limited test set [24, 18] and potential dataset leakage from modifications to the Sally-Anne task [3] in [5, 18, 24], suggest caution in the interpretation of these results (see App. D for a detailed discussion)
